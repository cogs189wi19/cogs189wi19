{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 189: EEG-Based Brain Computer Interfaces\n",
    "## Assignment 1: Intro to EEG Pre-Processing\n",
    "Created By: Alessandro \"Ollie\" D'Amico\n",
    "***\n",
    "**Assigned: 1/19/2019** <br>\n",
    "**Due: 1/25/2019 at 11:59 PM**\n",
    "***\n",
    "## Overview:\n",
    "For this assignment, we will be going over the basics of EEG processing and filtering. The dataset we will be working with is synthetic and was generated to easily visualize differences between target and non-target stimuli which have been corrupted with noise.<br>\n",
    "### Oddball Task:\n",
    "Standard visual oddball tasks involve a subject looking at stimuli being presented on a screen. One stimuli is more common than the other. The subject is instructed to count how many times they see the infrequent stimuli. When the participant sees the infrequent stimulus, a response is elicited which can be picked up via EEG. The name of the component elicited by the oddball is known as the visual **P300** or **P3** <br>\n",
    "***\n",
    "***Question 1:*** What does the P in P300 stand for? <br>\n",
    "***Question 2:*** What does the 300 part of the P300 refer to? <br>\n",
    "***Question 3:*** Is the P300 only generated by visual tasks? <br>\n",
    "***\n",
    "### EEG Data:\n",
    "The EEG data collected was sampled at 500 Hz. For those unfamiliar: <br>\n",
    "\\begin{equation*}\n",
    "Hz = \n",
    "\\frac{1}{second}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "Which means we collect 500 points of data per second at a data collection rate of 500 Hz.\n",
    "<br>\n",
    "This is referred to as the **sampling rate**, and is often denoted as \\begin{equation*}\n",
    "F_s \\end{equation*} *Fs* or *fs* is commonly used when writing code.\n",
    "<br>\n",
    "For this dataset, we will be analyzing data collected from site CPz, which was the site that produced the maximal P300 for this particular subject. 3 other sites were recorded from, but for simplicity, we will only look at a single channel.\n",
    "***\n",
    "***Question 4:*** How many milliseconds (ms) are there between each data point when the sampling rate is 200 Hz? \n",
    "<br>*(Hint: At 1000 Hz, there is 1ms between each data point. At 250 Hz, there is 4ms between each data point.)*<br>\n",
    "***\n",
    "## Section 1: Setup\n",
    "Before we can begin processing our data, we must first import some useful packages. <br>\n",
    "- **numpy** is used widely for processing numerical data, and supports matrix operations natively. We will be using NumPy arrays to store our data.\n",
    "- **scipy** contains useful functions to calculate filter coefficients and carry out filtering. We will be using SciPy filters to clean our EEG data.\n",
    "- **matplotlib** is used to create plots. We will use it to visually explore our filter and EEG data.\n",
    "\n",
    "For those unfamiliar with Jupyter, you must click on the block of code you wish to run, and click the ***Run*** button in the upper toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      # for dealing with data\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz  # for filtering\n",
    "import matplotlib.pyplot as plt                         # for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define some functions for filtering. The specifics of the filters will be explained in the next section. The code used for these functions was taken and subsequently adapted from:\n",
    "https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit for these functions goes to 'WarrenWeckesser'\n",
    "# https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order = 2):\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        sos = butter(order, [low, high], analog = False, btype = 'band', output = 'sos')\n",
    "        return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order = 2):\n",
    "        sos = butter_bandpass(lowcut, highcut, fs, order = order)\n",
    "        y = sosfiltfilt(sos, data)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Filter Design\n",
    "For EEG data analysis, there are various decisions that need to be made when selecting an appropriate filter. All signals are corrupted by outside noise, and EEG data is particularly susceptible to noise from electronics, muscle movements, and skin conductance changes (sweat). Filtering allows us to get rid of some of the junk, but it is not magic. There is no substitute for collecting clean data. Filtering will also throw out some of the data we care about, but this is a relatively small price to pay. <br><br>\n",
    "When selecting a filter, it is often a good idea to review the relevant literature. <br><br>\n",
    "For this assignment, we will be using a standard filter for ERP research known as the Butterworth filter. Butterworth filters aim to remove (attenuate) frequencies which you are not interested in, while leaving the frequencies you are interested in alone. We will be constructing a Butterworth *bandpass* filter. Bandpass filters allow a certain range of frequencies to *pass* through, while other frequencies are attenuated (the opposite of amplified). <br><br>\n",
    "Filter construction requires knowledge of the sampling rate, along with the lower and upper thresholds of frequencies to pass, and an order. We will be choosing **0.1 Hz** as our lowest frequency to pass, and **10 Hz** as our highest frequency to pass. For more information about this type of filter, please refer to: https://github.com/lucklab/erplab/wiki/Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtering variables\n",
    "fs = 500.0     # 500 Hz sampling rate\n",
    "lowcut = 0.1   # 0.1 Hz is the lowest frequency we will pass\n",
    "highcut = 10.0 # 10  Hz is the highest frequency we will pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will investigate the effect order has on the Butterworth filter. Ideally, we would keep the frequencies between 0.1-30 Hz alone while setting everything else to 0. Here is an idealized filter:\n",
    "***\n",
    "***Question 5:*** Modify the temporary variables in the next block of code so that the filter passes frequencies between 20 Hz and 40 Hz. Take a snapshot of the resulting plot as your answer to this question.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Question 5 --\n",
    "low_cut  = 0.1; # EDIT THIS VARIABLE\n",
    "high_cut = 10.0; # EDIT THIS VARIABLE\n",
    "\n",
    "# Plot an ideal filter \n",
    "plt.figure(0);\n",
    "plt.clf();\n",
    "\n",
    "# Draw out the filter (you can ignore all of this code, just run it)\n",
    "plt.plot([0.0,     low_cut],  [0, 0], 'b-')\n",
    "plt.plot([low_cut,  low_cut],  [0, 1], 'b-')\n",
    "plt.plot([low_cut,  high_cut], [1, 1], 'b-')\n",
    "plt.plot([high_cut, high_cut], [1, 0], 'b-')\n",
    "plt.plot([high_cut, 100.0],   [0, 0], 'b-', label=\"ideal filter\")\n",
    "plt.grid(True)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Gain')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the x-axis represents the frequencies in the signal and the y-axis is the gain of the filter. A gain of 1 means that signals of that frequency are passed at the same magnitude. A gain of 0 means signals at those frequencies are completely removed.\n",
    "<br><br>\n",
    "However, this idealized filter has several drawbacks. Data which pass through this filter may have their phase\n",
    "heavily distorted. There is a tradeoff between how sharply you can dissect signals in time and frequency. If you make sharp\n",
    "changes in frequency you will distort the time-domain waveform and vice-versa.In order to get the benefit of this 'brickwall' design, the Butterworth filter 'rounds' the corners, which ameliorates these distortions. \n",
    "<br><br>\n",
    "In the Butterworth filter, the 'sharpness' of the corners is determine by the order of the filter. Higher orders approach an idealized filter. Let's explore this concept by plotting increasing orders of Butterworth filters below.\n",
    "***\n",
    "***Question 6:*** Modify the *orders* variable to include values 8, 10, and 60. Run the code, and take a snapshot of the plot as the answer to this question.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the effects of order on the Butterworth filter\n",
    "# This code was adapted from https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "orders = [2, 4, 6]; # -- Question 6 --\n",
    "\n",
    "for order in orders:\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order = order)\n",
    "    w, h = sosfreqz(sos, worN = 2000)\n",
    "    plt.plot((fs * 0.5 / np.pi) * w, abs(h), label = \"order = %d\" % order)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Gain')\n",
    "plt.grid(True)\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***Question 7:*** Besides the corner becoming sharper, what is an observable effect of increasing the order?\n",
    "***\n",
    "## Section 3: EEG Data\n",
    "We will now load in and process the EEG data. There are several aspects to pre-processing which we will cover:\n",
    "- Filtering\n",
    "- Epoching\n",
    "- **Grouping**\n",
    "- Baseline correcting\n",
    "- Averaging\n",
    "\n",
    "***Note: In class, Professor de Sa mentioned the concept of 'binning' as taking the average of data points around each other. In ERP research, binning may also refer to grouping certain trials together based on their category. In order to avoid confusion, in this class we will be using the term grouping to refer to combining like-data together*** \n",
    "\n",
    "We will examine the data at each of these stages so that the effects of each step becomes intuitive. First, let's load in the data. The data are broken up into 3 parts:\n",
    "- **raw_eeg_df** contains our continous EEG data from channel CPz\n",
    "- **time_df** contains the time points (in ms)\n",
    "- **mark_df** contains our markers and when they occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of our datasets                           \n",
    "raw_eeg_df  = np.genfromtxt('Data_CPz_df.csv', delimiter=','); # Continuous EEG data (uV) from site CPz\n",
    "time_df     = np.genfromtxt('Time_df.csv', delimiter=',');     # Time points (ms) of each sample above\n",
    "mark_df     = np.genfromtxt('Markers_df.csv', delimiter=',');  # Markers and when they occurred in time (ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some of the basic properties of our dataset. *raw_eeg_df* should have the same dimensions as *time_df*. We can explore this by looking at the shape of the arrays. We will also plot our continuous data to build some intuition of how much time our data span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the shape of our data\n",
    "print(raw_eeg_df.shape)\n",
    "print(time_df.shape)\n",
    "\n",
    "# Plot the continuous data\n",
    "plt.plot(raw_eeg_df[:], label = \"continuous raw eeg\");\n",
    "plt.xlabel('Samples');\n",
    "plt.ylabel('Amplitude (uV)');\n",
    "plt.grid(True);\n",
    "plt.legend(loc = 'best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***Question 8:*** How much time is contained in this continuous dataset? *(Hint: Use your answer from Question 4 and the shape of time_df)*\n",
    "***\n",
    "Now we will examine *mark_df*, which has 2 columns. The first contains information about which stimulus the participant (or in this case a simulation) saw.\n",
    "- **0** indicates the stimulus was a target (the infrequent or oddball stimulus)\n",
    "- **1** indicates the stimulus was a non-target (the frequent stimulus)\n",
    "\n",
    "\n",
    "The second column contains the precise time the stimulus was shown. Let's take a look at the first six data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress = True) # To not have scientific notation\n",
    "mark_df[0:6, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***Question 9:*** How many trials (i.e. number of markers) do we have? <br>\n",
    "***Question 10:*** How many target trials do we have?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Question 9 --\n",
    "# WRITE YOUR CODE HERE (Hint: we're looking for the dimensions of the dataframe)\n",
    "\n",
    "# -- Question 10 --\n",
    "# COMPLETE THE FOLLOWING CODE\n",
    "target_code = 1337; # <- CHANGE THIS VALUE (Hint: use the discription of mark_df above)\n",
    "np.count_nonzero(mark_df[:, 0] == target_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our data exploration by examining the effect of filtering our data.<br>\n",
    "Let's look at a random section of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random 300-sample chunk of data\n",
    "plt.plot(raw_eeg_df[600:900], label = \"raw eeg\");\n",
    "plt.xlabel('Samples');\n",
    "plt.ylabel('Amplitude (uV)');\n",
    "plt.grid(True);\n",
    "plt.legend(loc = 'best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these data appear to be contaminated by high frequency noise. <br><br>\n",
    "\n",
    "Let's now apply our Butterworth filter to the entirety of our *raw_eeg_df* and save it as *clean_eeg_df*. <br><br>\n",
    "\n",
    "As a reminder, we will be passing the following arguments into our function:\n",
    "- **data:** our raw_eeg_df data\n",
    "- **lowcut:** the lowest frequency we will allow (0.1 Hz)\n",
    "- **highcut:** the highest frequency we will allow (30 Hz)\n",
    "- **fs:** our sampling rate (200 Hz)\n",
    "- **order:** the order of our filter\n",
    "\n",
    "For this assignment, we will be defining *order = 2*. 2 is commonly used for P300 and other ERPs, so we will stick with it as the literature validates its selection. Note that since we are applying a non-causal filter, the effective filter order is actually doubled since the filter is being applied twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter our raw_eeg_df\n",
    "order = 2;\n",
    "clean_eeg_df = butter_bandpass_filter(raw_eeg_df, lowcut, highcut, fs, order)\n",
    "\n",
    "# And now let's plot the same 300-sample chunk\n",
    "plt.plot(clean_eeg_df[600:900], label = \"clean eeg\");\n",
    "plt.xlabel('Samples');\n",
    "plt.ylabel('Amplitude (uV)');\n",
    "plt.grid(True);\n",
    "plt.legend(loc = 'best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** The filter used for these data are *noncausal* or *acausal*. *Causal* filters can be thought of as real-time, meaning that they only take inputs from the past and present. Noncausal filters, on the otherhand, use future data as well, which can only be used in offline implementations.\n",
    "<br><br>\n",
    "The data look considerably better, and that higher frequency noise has been heavily attenuated. Now that we have filtered all of our EEG data, we can begin to epoch the clean data!\n",
    "<br><br>\n",
    "Epoching is the step of pre-processing where we segment the data into the chunks we care about. We are only interested in the brief periods of time the participant has seen the stimulus. When epoching data, we must decide how much data is worth keeping. Since the P300 occurrs between 300-400ms after the onset of a stimulus, we know we need to go out atleast 300ms after our marker times. We also need to save some data before the onset of the stimulus so that we can perform *baseline correction*. <br><br>\n",
    "\n",
    "Baseline correction involves selecting a period of time where there is little to no neuronal activity of interest occurring. We can take the average of this 'nothingness' period and subtract it from all samples in our epoch so that the baseline has zero mean. This is important because variability in the EEG data makes it so that each epoch has a slightly different average amplitude. Without baseline correction, the amplitudes of peaks being averaged may cause false positives during data analysis, which is something we obviously want to avoid. <br><br>\n",
    "\n",
    "Since we want our baseline period to contain as few interesting signals as possible, we usually look a few hundred milliseconds before the onset of the stimulus. For this analysis, we will be using a 100ms chunk of data which starts 400ms before the onset of the stimulus as our baseline. We can therefore start our epoch 400ms before the onset of the stimulus.\n",
    "<br><br>\n",
    "For the last point of the epoch, we want to make sure we capture the entire ERP of interest. We could safely go to about 500ms after the onset of the stimulus for the P300. <br><br>\n",
    "\n",
    "For the sake of clean plots, you will often see researchers slightly overshoot their epochs, and I will be doing the same. The starting time point will be 500ms before the onset of the stimulus, and the ending time point will be 1000ms after the onset of the stimulus. Let's define these variables, and create a new *epoched_df* to contain our epoched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounds of our epoch as well as our baseline\n",
    "epoch_s = -500; # 500ms before stimulus\n",
    "epoch_e = 1000; # 1000ms after stimulus\n",
    "bl_s    = -400; # 400ms before stimulus\n",
    "bl_e    = -300; # 300ms before stimulus\n",
    "dt      = (fs / 1000)\n",
    "\n",
    "# Let's calculate the length our epoch with our given sampling rate\n",
    "epoch_len = int((abs(epoch_s) + abs(epoch_e)) * dt);\n",
    "\n",
    "# And let's define our epoch_df to fit our needs. \n",
    "# Each row contains a unique epoch of data\n",
    "epoch_df = np.zeros(shape = (int(mark_df.shape[0]), epoch_len));\n",
    "\n",
    "# Let's print the shape of epoch_df so we can verify it's correct\n",
    "print(epoch_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created *epoch_df*, we can begin epoching our dataset. We will do this by using the times in *mark_df* to select the appropriate chunk of data, which we will store in *epoch_df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some helpful variables to make our extraction easier\n",
    "e_s = int(epoch_s * (fs / 1000)); # effectively the number of indices before marker we want\n",
    "e_e = int(epoch_e * (fs / 1000)); # effectively the number of indices after marker we want\n",
    "\n",
    "# Epoch the data\n",
    "for i in range(0, int(mark_df.shape[0])):\n",
    "    # This calculation of T needs to be explained for Wi21 course\n",
    "    # For those who stumbled to this page, find Wi21 course or take it as some practice\n",
    "    t = np.where((time_df < (mark_df[i, 1] + dt/1000)) & (time_df > (mark_df[i, 1] - dt/1000)))[0][0]\n",
    "    epoch_df[i, :] = clean_eeg_df[t + e_s:t + e_e] # grab the appropriate samples around the stimulus onset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's plot all of our epochs on the same plot. This will give us an excuse to define an array of time points for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our time points\n",
    "dt = int(round(1 / (fs / 1000))); # note: this is the same as our 'rounder' variable from earlier\n",
    "times = range(epoch_s, epoch_e, dt)\n",
    "\n",
    "# Shenanigous plotting\n",
    "for i in range(0, int(epoch_df.shape[0])):\n",
    "    plt.plot(times, epoch_df[i, :]);\n",
    "    \n",
    "plt.xlabel('Time (ms)');\n",
    "plt.ylabel('Amplitude (uV)');\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell, there is great variability in the ERP in every trial. \n",
    "<br><br>\n",
    "We will now baseline correct our epoched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some helpful variables\n",
    "b_s = int((abs(epoch_s) + bl_s) * (fs / 1000)); # index in epoch_df where our baseline begins\n",
    "b_e = int((abs(epoch_s) + bl_e) * (fs / 1000)); # index in epoch_df where our baseline ends\n",
    "\n",
    "# Perform baseline correction\n",
    "for i in range(0, int(epoch_df.shape[0])):\n",
    "    epoch_df[i, :] = epoch_df[i, :] - np.mean(epoch_df[i, b_s:b_e]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now *group* our data. This means that we will segregate the non-target (1) and target (2) epochs by using *mark_df*. We will create  *norm_df* which will contain our non-target eeg dat, and *odd_df*, which will contain our target oddball data. The following method of grouping is only viable because the structure of *mark_df* and *epoch_df* are such that the rows correspond to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first grab which indices contain a 1 so that we can create our dataframes in one move\n",
    "norm_ix = (np.isin(mark_df[:, 0], 1)); # creates a boolean matrix, returning true for target trials\n",
    "\n",
    "# Now let's group our data\n",
    "norm_df = epoch_df[ norm_ix, :]; # normal epochs contain a 1\n",
    "odd_df  = epoch_df[~norm_ix, :]; # oddball epochs do not contain a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create an average waveform for *norm_df* and *odd_df* and plot our final waveforms!\n",
    "<br><br>\n",
    "We will be plotting positive up, although it's common to see negative plotted up as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our average waveforms\n",
    "norm_ave = np.mean(norm_df, axis = 0);\n",
    "odd_ave  = np.mean(odd_df,  axis = 0);\n",
    "\n",
    "# Plot our waveforms\n",
    "plt.plot(times, odd_ave, label =  \"target\");\n",
    "plt.plot(times, norm_ave, label = \"non-target\");\n",
    "plt.xlabel('Time (ms)');\n",
    "plt.ylabel('Amplitude (uV)');\n",
    "plt.grid(True);\n",
    "plt.legend(loc = 'best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***Question 11:*** Take a snapshot of this plot as the answer for this question\n",
    "***\n",
    "***Note:*** The monitor used to collect these data had a delay of about 80ms, which was not corrected in this analysis. It is best practice to correct for these delays. The delay was determined using a photosensor. Correcting for delay is an important part of precise ERP research and it is best practice to correct for these delays. It is relatively simple to do on the continuous data, but to avoid confusion it has been left out of this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
